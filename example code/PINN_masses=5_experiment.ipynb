{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdE9ShALJfLU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from scipy.integrate import solve_ivp\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGu8reYjGtCX"
      },
      "outputs": [],
      "source": [
        "from functorch import vmap, vjp\n",
        "from functorch import jacrev, jacfwd\n",
        "\n",
        "class NNApproximator(nn.Module):\n",
        "  def __init__(self, dim_input = 1, dim_output = 2, num_hidden = 2, dim_hidden = 1, activation=nn.Tanh()):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layer_in = nn.Linear(dim_input, dim_hidden)\n",
        "    self.layer_out = nn.Linear(dim_hidden, dim_output)\n",
        "    # self.A = nn.Parameter(torch.randn(2,2))\n",
        "    self.k = nn.Parameter(torch.rand(1, requires_grad=True))\n",
        "    # self.A = self.k * torch.from_numpy(np.array([[-1,1],[1,-1]]))\n",
        "\n",
        "    num_middle = num_hidden - 1\n",
        "    self.middle_layers = nn.ModuleList(\n",
        "        [nn.Linear(dim_hidden, dim_hidden) for _ in range(num_middle)]\n",
        "    )\n",
        "    self.activation = activation\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.activation(self.layer_in(x))\n",
        "    for layer in self.middle_layers:\n",
        "      out = self.activation(layer(out))\n",
        "    return self.layer_out(out)\n",
        "\n",
        "  # reference for implementing derivatives for batched inputs\n",
        "  # https://pytorch.org/functorch/stable/notebooks/jacobians_hessians.html\n",
        "  def jacobian(self, x):\n",
        "    jac = vmap(jacrev(self.forward))\n",
        "    return jac(x).squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_data_loss(model, x_tr, y_tr):\n",
        "  return 0.5 * torch.mean((model.forward(x_tr) - y_tr) ** 2)\n",
        "\n",
        "def compute_PINN_loss(model, x, k):\n",
        "    F_dot = model.jacobian(x)\n",
        "    s1 = x[:, 0:2] - x[:, 2:4]\n",
        "    s2 = x[:, 4:6] - x[:, 2:4]\n",
        "    return ((k - torch.norm(torch.einsum('ijk,ik->ij', F_dot[:, :, 0:2], s1), dim=1) / torch.norm(s1, dim=1)) ** 2).mean() + ((k - torch.norm(torch.einsum('ijk,ik->ij', F_dot[:, :, 4:6], s2), dim=1) / torch.norm(s2, dim=1)) ** 2).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "in_dim = 6\n",
        "out_dim = 2\n",
        "num_layer = 5\n",
        "hidden_dim = 32\n",
        "xy_index = [13, 14, 1, 2, 5, 6]\n",
        "epoch = 20000\n",
        "lr = 1e-4\n",
        "\n",
        "data_file = '../data_set_bis.npy'\n",
        "\n",
        "x = torch.from_numpy(np.load(data_file)).float()\n",
        "x_PINN = x.clone().requires_grad_(True)\n",
        "F = x[:, [3, 4]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHaj8orCJziH"
      },
      "outputs": [],
      "source": [
        "def train_model(model, data_loss_fn, PINN_loss_fn, learning_rate=0.0001, max_epochs=1000):\n",
        "  tr_losses = []\n",
        "\n",
        "  # reference on torch.LBFGS usage\n",
        "  # https://gist.github.com/tuelwer/0b52817e9b6251d940fd8e2921ec5e20\n",
        "  USE_BFGS = False\n",
        "\n",
        "  if USE_BFGS:\n",
        "    optimizer = torch.optim.LBFGS(model.parameters())\n",
        "    print(\"Using BFGS optimizer ... \")\n",
        "    log_iter = 10\n",
        "    def closure():\n",
        "        optimizer.zero_grad()\n",
        "        objective = data_loss_fn(model) + PINN_loss_fn(model)\n",
        "        objective.backward()\n",
        "        return objective\n",
        "  else:\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "    print(\"Using Adam optimizer ... \")\n",
        "    log_iter = 1000\n",
        "\n",
        "  for epoch in range(max_epochs):\n",
        "    loss = data_loss_fn(model) + PINN_loss_fn(model)\n",
        "    if USE_BFGS:\n",
        "      optimizer.step(closure)\n",
        "    else:\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    if epoch % log_iter == 0:\n",
        "      print(f\"Epoch: {epoch} - Loss: {float(loss):>7f}\")\n",
        "      print(f\"Loss over entire dataset: {float(compute_data_loss(model, x[:, xy_index], F).detach()):>7f}\")\n",
        "    tr_losses.append(loss.detach().numpy())\n",
        "\n",
        "  return model, np.array(tr_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = NNApproximator(dim_input=in_dim, dim_output=out_dim, num_hidden=num_layer, dim_hidden=hidden_dim)\n",
        "\n",
        "model, tr_losses = train_model(\n",
        "    model,\n",
        "    data_loss_fn=lambda model: compute_data_loss(model, x[:500, xy_index], F[:500]),\n",
        "    PINN_loss_fn=lambda model: 1e-1*compute_PINN_loss(model, x_PINN[:, xy_index], 50),\n",
        "    learning_rate=lr,\n",
        "    max_epochs=epoch,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getqdot(x, xdot, model, xLeft, xRight):\n",
        "    N_m = x.shape[0] // 2\n",
        "\n",
        "    x = np.concatenate((xLeft, x, xRight))\n",
        "    forces = []\n",
        "    for i in range(0, N_m):\n",
        "        triplet = x[np.arange(2*i, 2*i + 6)]\n",
        "        force = model.forward(torch.from_numpy(triplet).float())\n",
        "        forces.append(force.detach().numpy())\n",
        "        # force = getSpringForcesOnMass(triplet[:2],triplet[2:4],triplet[4:])\n",
        "        # forces.append(force)\n",
        "\n",
        "    forces = np.concatenate(forces).reshape(-1)\n",
        "\n",
        "    q0 = np.concatenate((x, xdot))\n",
        "    qdot = np.concatenate((xdot, forces))\n",
        "    return qdot\n",
        "\n",
        "def compute_trajectory(x0, x0dot, model, xLeft, xRight):\n",
        "    N_m = x0.shape[0] // 2\n",
        "    q0 = np.concatenate((x0, x0dot))\n",
        "\n",
        "    t0 = 0\n",
        "    tf = 20\n",
        "    Nt = 101\n",
        "    sol = solve_ivp(lambda t, q: getqdot(q[:2*N_m], q[2*N_m:], model, xLeft, xRight), [t0,tf], y0=q0, t_eval=np.linspace(t0, tf, Nt))\n",
        "    y = sol.y\n",
        "    t = sol.t\n",
        "    return y, t\n",
        "\n",
        "# modify as appropriate:\n",
        "N_m = 5\n",
        "xLeft = x[0, 13:15]\n",
        "xRight = x[0, 15:17]\n",
        "\n",
        "# # the vector 'x0' contains the initial positions of the *movable* masses\n",
        "# # i.e. x0.shape = [N_m * 2]\n",
        "x0 = x[0, [1, 2, 5, 6, 9, 10]]\n",
        "# x0 = np.c_[x0, np.zeros_like(x0)].reshape(-1)\n",
        "# print(x0)\n",
        "x0dot = np.zeros_like(x0)\n",
        "\n",
        "# model = lambda x: np.array([(x[1]-x[0]), (x[0]-x[1])])\n",
        "\n",
        "y, t = compute_trajectory(x0,x0dot,model,xLeft,xRight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(t, y.T[:,1], label='true tragetory')\n",
        "plt.plot(x[:, 0], x[:, 2], label='predict tragetory')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(x[:, 0], x[:, 3], label='true force - x')\n",
        "plt.plot(x[:, 0], model.forward(x[:, [13, 14, 1, 2, 5, 6]])[:, 0].detach().numpy(), label='predicted force - x')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(x[:, 0], x[:, 4], label='true force - y')\n",
        "plt.plot(x[:, 0], model.forward(x[:, [13, 14, 1, 2, 5, 6]])[:, 1].detach().numpy(), label='predicted force - y')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
